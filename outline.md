#### 1 Introduction 1
- 1.1 Machine Learning: what and why?
  - 1.1.1 Types of machine learningp2
- 1.2 Supervised learning 3
  - 1.2.1 Classification p3
  - 1.2.2 Regression p8
- 1.3 Unsupervised learning p9
  - 1.3.1 Discovering clusters p10
  - 1.3.2 Discovering latent factors p11
  - 1.3.3 Discovering graph structure p13
  - 1.3.4 Matrix completion p14
1.4 Some basic concepts in machine learning p16
  - 1.4.1 Parametric vs non-parametric models p16
  - 1.4.2 A simple non-parametric classifier: K-nearest neighbors p16
  - 1.4.3 The curse of dimensionality p18
  - 1.4.4 Parametric models for classification and regression p19
  - 1.4.5 Linear regression p19
  - 1.4.6 Logistic regression p21
  - 1.4.7 Overfitting p22
  - 1.4.8 Model selection p22
  - 1.4.9 No free lunch theorem 

#### 2 Probability 27
- 2.1 Introduction p27
- 2.2 A brief review of probability theory p28
  - 2.2.1 Discrete random variables p28
  - 2.2.2 Fundamental rules p29
  - 2.2.3 Bayes' rule p29
  - 2.2.4 Independence and conditional independence p31
  - 2.2.5 Continuous random variables p32
  - 2.2.6 Quantiles p33
  - 2.2.7 Mean and variance p33
- 2.3 Some common continuous distributions p34
  - 2.3.1 The binomial and Bernoulli distributions p34
  - 2.3.2 The multinomial and multinoulli distributions p35
  - 2.3.3 The Poisson distribution p37
  - 2.3.4 The empirical distribution p37
- 2.4 Some common continuous distributions p38
  - 2.4.1 Gaussian (normal) distribution p38
  - 2.4.2 Degenerate pdf p39
  - 2.4.3 The student's t distribution p39
  - 2.4.4 The Laplace distribution p41
  - 2.4.5 The gamma distribution p41
  - 2.4.6 The beta distribution p43
  - 2.4.7 Pareto distribution p43
- 2.5 Joint probability distributions p44
  - 2.5.1 Covariance and correlation p45
  - 2.5.2 The multivariate Gaussian p46
  - 2.5.3 Multivariate student t distribution p47
  - 2.5.4 Dirichlet distribution p49
- 2.6 Transformation of random variables p49
  - 2.6.1 Linear transformations p49
  - 2.6.2 General transformatioins p50
  - 2.6.3 Central limit theorem p52
- 2.7 Monte Carlo approximation p53
  - 2.7.1 Example: change of variables, the MC way p53
  - 2.7.2 Example pi by Monte Carlo integration p54
  - 2.7.3 Accuracy of Monte Carlo approximation p54
- 2.8 Information theory p56
  - 2.8.1 Entropy p57
  - 2.8.2 KL divergence p589
  - 2.8.3 Mutual information p59

#### Generative models for discrete data p67
- 3.1 Introduction p67
- 3.2 Bayesian concept learning p67
  - 3.2.1 Likelihood p69
  - 3.2.2 Prior p69
  - 3.2.3 Posterior p70
  - 3.2.4 Posterior predictive distributioin p73
  - 3.2.5 A more complex prior p74
- 3.3 The beta-binomial model p74
  - 3.3.1 Likelihood p75
  - 3.3.2 Prior p76
  - 3.3.3 Posterior p77
  - 3.3.4 Posterior predictive distribution p79
- 3.4 The Dirichlet-multinomial model p80
  - 3.4.1 Likelihood p81
  - 3.4.2 Prior p81
  - 3.4.3 Posterior p81
  - 3.4.4 Posterior predictive p83
- 3.5 Naive Bayes classifiers p84
  - 3.5.1 Model fitting p85
  - 3.5.2 Using the model for prediction p87
  - 3.5.3 The log-sum-exp trick p88
  - 3.5.4 Feature selection using mutual information p89
  - 3.5.5 Classifying documents using bag of words p90


#### 3 Generative Models for Discrete Data 67

#### 4 Gaussian Models p99
- 4.1 Introduction p99
  - 4.1.1 Notation p99
  - 4.1.2 Basics p99
    - Algorithm 4.1
    - Fig 4.1 - Visualization of a 2 dimensional Gaussian 
  density p100
  - 4.1.3 MLE for an MVN p101
    - Proof 4.10 p101
  - 4.1.4 *Maximum entropy deviation of the Gaussian p103
- 4.2 Guassian discriminant analysis p103
  - Figure 4.2 (a) Height / weight data (b) Visualization of 2d Guassians fit to each class p104
  - 4.2.1 Quadratic discriminant analysis (QDA) p104
  - 4.2.2 Linear discriminant analysis (LDA) p105
  - 4.2.3. Two-class LDA p106
  - 4.2.4 MLE for discriminant analysis p108
  - 4.2.5 Strategies for preventing overfitting p108
  - 4.2.6 Regularized LDA * p109
  - 4.2.7 Diaganol LDA p110
  - 4.2.8 Nearest shrunken centroids classifier p111
- 4.3 Inference in jointly Gaussian distributions p112
  - 4.3.1 Statement of the result p113
  - 4.3.2 Examples p113
    - 4.3.2.1 Marginals and conditionals of a 2d Gaussian p113
    - 4.3.2.2 Interpolating noise-free data p114
    - 4.3.2.3 Data imputation p116
  - 4.3.3 Information form p117
  - 4.3.4 Proof of the result p118
    - 4.3.4.1 Inverse of a partitioned matrix using Schur complements p118
    - 4.3.4.2 The matrix invresion lemma p120
    - 4.3.4.3 Proof of Gaussian conditioning formulas p120
- 4.4 Linear Gaussian systems p121
  - 4.4.1 Statement of the result p122
  - 4.4.2 Examples p122
    - 4.4.2.1 Inferring an unknown scalar from noisy measurements p122
    - 4.4.2.2 Inferring an unknown vector from noisy measurments p124
    - 4.4.2.3 Interpolating noisy data p125
  - 4.4.3 *Proof of the result p127
- 4.5 *Digression: The Wishart distribution p128
  - 4.5.1 Inverse Wishart distribution p129
  - 4.5.2 Visualizing the Wishart distribution p129
- 4.6 Inferring the parameters of an MVN p129
  - 4.6.1 Posterior distribution of mu p130
  - 4.6.2 Posterior distribution of E p131
    - 4.6.2.1 MAP estimation p131
    - 4.6.2.2 Univariate posterior p133
  - 4.6.3 Posterior distribution of u and E p134
    - 4.6.3.1 Likelihood p134
    - 4.6.3.2 Prior p134
    - 4.6.3.3 Posterior p136
    - 4.6.3.4 Posterior mode p136
    - 4.6.3.5 Posterior marginals p136
    - 4.6.3.6 Posterior predictive p137
    - 4.6.3.7 Posterior for scalar data p138
    - 4.6.3.8 Bayesian t-test p139
    - 4.6.3.9 Connection with frequentist statistics p140
  - *4.6.4 Sensor fusion with unknown properties p140

#### 5 Bayesian Statistics 151
- 5.1 Introduction p151
- 5.2 Summarizing posterior distributions p151
  - 5.2.1 MAP estimation p151
    - 5.2.1.1 No measure of uncertainty p152
    - 5.2.1.2 Plugging in the MAP can result in overfitting p152
    - 5.2.1.3 The mode is an untypical point p152
    - 5.2.1.4 Map estimation is not invariant to reparameterizatin p153
  - 5.2.2 Credible intervals p154
    - 5.2.2.1 *Highest posterior desnity regions p155
  - 5.2.3 Inference for a difference in porportions p156
- 5.3 Bayesian model selection p157
  - 5.3.1 Bayesian Occam's razor p158
  - 5.3.2 Computing the marginal likelihood (evidence) p160
    - 5.3.2.1 Beta-binomial model p162
    - 5.3.2.2 Dirichlet-multinoulli model p162
    - 5.3.2.3 Gaussian-Wishart-Gaussian model p163
    - 5.3.2.4 BIC approximation to log marginal likelihood p163
    - 5.3.2.5 Effect of the prior p164
  - 5.3.3 Bayes factors p165
    - 5.3.3.1 Example: Testing if a coin is fair p165
  - 5.3.4 *Jeffrey's-Lindley paradox p166
- 5.4 Priors p167
  - 5.4.1 Uninformative priors p167
  - 5.4.2 Jeffrey's priors p168
    - 5.4.2.1 Example: Jeffrey's prior for the Bernoulli adn multinoulli p169
    - 5.4.2.2 Example: Jeffrey's prior for the location and scale parameters p170
  - 5.4.3 Robust priors p170
  - 5.4.4 Mixture of conjugate priors p171
    - 5.4.4.1 Example p171
    - 5.4.4.2 Application: Finding conserved regions in DNA and protein sequences p172
- 5.5 Hierarchical Bayes p174
  - 5.5.1 Example: modeling related cancer rates p173
- 5.6 Emperical Bayes p174
  - 5.6.1 Example: beta-binomial model p175
  - 5.6.2 Example: Gaussian-Gaussian model p176
    - 5.6.2.1 Example: predicting baseball scores p176
    - 5.6.2.2 Estimating the hyper-parameters p178
- 5.7 Bayesian decision theory p178
  - 5.7.1 Bayes estimators for common loss functions p179
    - 5.7.1.1 MAP estimate minimized 0-1 loss p179
    - 5.7.1.2 Reject option p180
    - 5.7.1.3 Posterior mean minimizes l2 (quadratic) loss p180
    - 5.7.1.4 Posterior media minimizes l1 (absolute) loss p181
    - 5.7.1.5 Supervised learning p182
  - 5.7.2 The false positive vs false negative tradeoff p182
    - 5.7.2.1 ROC curves and all that p183
    - 5.7.2.2 Precision recall curves p184
    - 5.7.2.3 *F-scores p185
    - 5.7.2.4 *False discovery rates p186
  - 5.7.3 Other topics  * p186
    - 5.7.3.1 Contextual bandits p186
    - 5.7.3.2 Utility theory p187
    - 5.7.3.3 Sequential decision theory p188

#### 6 Frequentitist Statistics p193
- 6.1 Introduction p193
- 6.2 Sampling distribution of an estimator p193
  - 6.2.1 Bootstrap p194
  - 6.2.2 Large sample theory for the MLE * p195
- 6.3 Frequentist decision theory p197
  - 6.3.1 Bayes risk p197
  - 6.3.2 Minmax risk p198
  - 6.3.3 Admissible estimators p199
    - 6.3.3.1 Example p199
    - 6.3.3.2 Stein's paradox * p201
    - 6.3.3.3 Admissibility is not enough p202
- 6.4 Desirable properties of estimators p202
  - 6.4.1 Consistent estimators p202
  - 6.4.2 Unbiased estimators p203
  - 6.4.3 Minimum variance estimators p203
  - 6.4.4 The bias variance tradeoff p204
    - 6.4.4.1 Example: estimating a Gaussian mean p205
    - 6.4.4.2 Example: ridge regression p206
    - 6.4.4.3 Bias-variance tradeoff for classification p207
- 6.5 Emperical risk minimization p207
  - 6.5.1 Regularized risk minimization p208
  - 6.5.2 Structural risk minimization p208
  - 6.5.3 Estimating the risk using cross validation p209
    - 6.5.3.1 Example: using CV to pick lambda for ridge regression p210
    - 6.5.3.2 The one standard error rule p210
    - 6.5.3.3 CV for model selection in non-probabilistic unsupervised learning p211
  - 6.5.4 *Upper-bounding the risk using statistical learning theory p211
  - 6.5.5 Surrogate loss functions p213
- 6.6 *Pathologies of frequentist statistics p214
  - 6.6.1 Counter-intuitive behavior of confidence intervals p214
  - 6.6.2 p-values considered harmful p215
  - 6.6.3 The likelihood principle p217
  - 6.6.4 Why isn't everyone a Bayesian? p217

#### 7 Linear Regression p219
- 7.1 Introduction p219
- 7.2 Model specification p219
- 7.3 Maximum likelihood estimation (least squares) p219
  - 7.3.1 Derivation of the MLE p221
  - 7.3.2 Geometric interpretation p222
  - 7.3.3 Convexity p223
- 7.4 *Robust linear regression p225
- 7.5 Ridge regression p227
  - 7.5.1 Basic idea p227
  - 7.5.2 *Numerically stable computation p229
  - 7.5.3 *Connection with PCA p230
  - 7.5.4 Regularization effects of big data p232
- 7.6 Bayesian linear regression p233
  - 7.6.1 Computing the posterior p234
  - 7.6.2 Computing the posterior predictive p235
  - 7.6.3 *Bayesian inference when is unknown p236
    - 7.6.3.1 Conjugate prior p236
    - 7.6.3.2 Uninformative prior p238
    - 7.6.3.3 *An example where Bayesian and frequentist inference coincide p239
  - 7.6.4 EB for linear regression (evidence procedure) p240


#### 8 Logistic Regression p247
- 8.1 Introduction p247
- 8.2 Model specification p247
- 8.3 Model fitting p248
  - 8.3.1 MLE p249
  - 8.3.2 Steepest descent p249
  - 8.3.3 Newton's method p251
  - 8.3.4 Iteratively reweighted least squares (IRLS) p253
  - 8.3.5 Quasi-Newton (variable metric) methods p253
  - 8.3.6 l2 regularization p254
  - 8.3.7 Multi-class logistic regression p255
- 8.4 Bayesian logistic regression p257
  - 8.4.1 Laplace approximation p257
  - 8.4.2 Derivation of the Bayesian information criterion (BIC) p258
  - 8.4.3 Gaussian approximation for logistic regression p258
  - 8.4.4 Approximating the posterior predictive p260
    - 8.4.4.1 Monte Carlo approximation p260
    - 8.4.4.2 Probit approximation (moderated output) p261
  - 8.4.5 *Residual analysis (outlier detection) p263
- 8.5 Online learning and stochastic optimization p264
  - 8.5.1 Online learning and regret minimization p264
  - 8.5.2 Stochastic optimization and risk minimization p265
    - 8.5.2.1 Setting the step size p265
    - 8.5.2.2 Per-parameter step sizes p266
    - 8.5.2.3 SGD compared to batch learning p266
  - 8.5.3 The LMS Algorithm p267
  - 8.5.4 The perceptron algorithm p268
  - 8.5.5 A Bayesian view p270
- 8.6 Generative vs discriminitive classifiers p270
  - 8.6.1 Pros and cons of each approach p271
  - 8.6.2 Dealing with missing data p271
    - 8.6.2.1 Missing data at test time p273
    - 8.6.2.2 Missing data at training time p274
  - 8.6.3 *Fisher's linear discriminant analysis (FLDA) p274
    - 8.6.3.1 Derivation of the optimal 1d projection p274
    - 8.6.3.2 Extension to higher dimensions and multiple classes p277
    - 8.6.3.3 *Probabilistic interpretation of FLDA p278

#### 9 Generalized Linear models and the exponential family p283
- 9.1 Introduction 
- 9.2 The Exponential family p283
- 9.3 Generalized linear models (GLM) p292
- 9.4 Probit regression p295
- 9.5 Multi-task learning p298
- 9.6 *Generalized linear mixed models p300
- 9.7 *Learning to rank p302


#### 10 Directed Graphical Models (Bayes nets) p309
- 10.1 Introduction p309
- 10.2 Examples p313
- 10.3 Inference p321
- 10.4 Learning p322
- 10.5 Conditional independence properties of DGMs p326
- 10.6 *Influence (decision) diagrams p330